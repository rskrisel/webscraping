{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rskrisel/webscraping/blob/main/web_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67a7ca8b",
      "metadata": {
        "id": "67a7ca8b"
      },
      "source": [
        "# Web Scraping in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0df0605",
      "metadata": {
        "id": "d0df0605"
      },
      "source": [
        "In this workshop, we will learn how to retrieve text data using web scraping methods.\n",
        "\n",
        "</br>\n",
        "\n",
        "We will start by retrieving data from a single URL, then we will iterate this process across a list of URLs.\n",
        "\n",
        "</br>\n",
        "\n",
        "We will then clean our text data and visualize our results using Word Clouds and Lexical Density."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f3f7b55",
      "metadata": {
        "id": "1f3f7b55"
      },
      "source": [
        "### Acknowledgements"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ffd756",
      "metadata": {
        "id": "39ffd756"
      },
      "source": [
        "This workshop is adapted from the following tutorials:\n",
        "   - Martin Breuss, _Real Python_, [Beautiful Soup: Build a Web Scraper With Python](https://realpython.com/beautiful-soup-web-scraper-python/)\n",
        "   - Brannon Seay, _Code X_, [A Beginner’s Guide to Easily Create a Word Cloud in Python](https://medium.com/codex/a-beginners-guide-to-easily-create-a-word-cloud-in-python-7c3078c705b7)\n",
        "   - Melanie Walsh, _Introduction to Cultural Analytics_, Web Scraping  parts [I](https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/02-Web-Scraping-Part1.html) & [II](https://melaniewalsh.github.io/Intro-Cultural-Analytics/04-Data-Collection/03-Web-Scraping-Part2.html)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c3665d",
      "metadata": {
        "id": "04c3665d"
      },
      "source": [
        "## What Is Web Scraping?\n",
        "Web scraping is the process of gathering information from the Internet. </br>\n",
        "Manually copying and pasting information from a website is a form of web scraping! </br>\n",
        "However, “web scraping” usually involves automation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b84932",
      "metadata": {
        "id": "13b84932"
      },
      "source": [
        "Some websites don't mind web scraping while others have explicit terms of use against it (including most social media websites!) so always do your due diligence before scraping a website!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27ec5da",
      "metadata": {
        "id": "d27ec5da"
      },
      "source": [
        "### Why use Web Scraping for Text Analysis?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0efbabb",
      "metadata": {
        "id": "f0efbabb"
      },
      "source": [
        "Web scraping is an essential data collection method in the text analysis toolbox.\n",
        "It allows researchers to automate the collection of text data directly from websites that can then be used for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d5a3dbd",
      "metadata": {
        "id": "3d5a3dbd"
      },
      "source": [
        "### Drawbacks of Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d09c131",
      "metadata": {
        "id": "1d09c131"
      },
      "source": [
        "Data collected through web scraping is considered unstructured. It will exist as a disorganized string of letters and numbers. It is up to the researcher to organize the collected data, generally in tabular format, but not always."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba8e432c",
      "metadata": {
        "id": "ba8e432c"
      },
      "source": [
        "## Installing the Necessary Python Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e5a4cb",
      "metadata": {
        "id": "94e5a4cb"
      },
      "source": [
        "The successfully to complete the workshop, you need the following libraries:\n",
        "- **Requests** for making data requests from URLs (installation required)\n",
        "- **BeautifulSoup** for cleaning up and decoding HTML text data (installation required)\n",
        "- **Pandas** for visualizing and manipulating tabular data (comes standard with Anaconda)\n",
        "- **NLTK** for text normalizing and cleaning (comes standard with Anaconda, but an additional feature needs to be installed)\n",
        "- **Glob** to connect to directories on your OS (comes standard with Python 3, no download necessary)\n",
        "- **Word Cloud** to create visual representation of word frequencies (download required)\n",
        "- **Matplotlib** to visualize the Word Clouds (comes standard with Python 3, no download necessary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up our workspace\n",
        "\n",
        "\n",
        "1.   Mount your Drive\n",
        "2.   Store your folder path in the `path` variable\n",
        "\n"
      ],
      "metadata": {
        "id": "k-6Xv8mNcQTu"
      },
      "id": "k-6Xv8mNcQTu"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rSl380X7cej0"
      },
      "id": "rSl380X7cej0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'your path' #copy/paste your path"
      ],
      "metadata": {
        "id": "TTDh0Iu5cuWz"
      },
      "id": "TTDh0Iu5cuWz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "20c8b5aa",
      "metadata": {
        "id": "20c8b5aa"
      },
      "source": [
        "### Installing the NLTK Wordnet Dictionary\n",
        "\n",
        "The WordNet English dictionary is part of the Natural Language Tool Kit (NLTK) in Python.\n",
        "\n",
        "Run the following code directly from your Notebook:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk"
      ],
      "metadata": {
        "id": "E_GKqNnqbYpf"
      },
      "id": "E_GKqNnqbYpf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "metadata": {
        "id": "mHLl4hDabrgD"
      },
      "id": "mHLl4hDabrgD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0cdbbaa",
      "metadata": {
        "collapsed": true,
        "id": "e0cdbbaa"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "133e8543",
      "metadata": {
        "id": "133e8543"
      },
      "source": [
        "## Web Scraping from a single URL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c5321a6",
      "metadata": {
        "id": "7c5321a6"
      },
      "source": [
        "### Responses and Requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09983483",
      "metadata": {
        "id": "09983483"
      },
      "source": [
        "When you type in a URL in your search address bar, you’re sending an HTTP request for a web page, and the server which stores that web page will accordingly send back a response, some web page data that your browser will render."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0250eeb",
      "metadata": {
        "id": "f0250eeb"
      },
      "source": [
        "The process of connecting to a URL link for web scraping is similar. We use the \"request\" library to connect to the data stored within a URL."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d199306",
      "metadata": {
        "id": "1d199306"
      },
      "source": [
        "To start, we need to bring in our \"requests\" library into our Python environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf3c78e",
      "metadata": {
        "id": "8bf3c78e"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1f3256",
      "metadata": {
        "id": "4a1f3256"
      },
      "source": [
        "Let's make our first request. This is a URL to an article on the Associated Press website:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e785f3b",
      "metadata": {
        "id": "5e785f3b"
      },
      "outputs": [],
      "source": [
        "response = requests.get(\"https://www.cnn.com/2024/09/27/us/eric-adams-nyc-mayor-arraignment/index.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33107af5",
      "metadata": {
        "id": "33107af5"
      },
      "source": [
        "Next, we can check to see whether or not the request was successful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6515941a",
      "metadata": {
        "id": "6515941a"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d8800c",
      "metadata": {
        "id": "83d8800c"
      },
      "source": [
        "We get a 200 status code, which means our request was successful! Read here for more on status codes: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c43507",
      "metadata": {
        "id": "e0c43507"
      },
      "source": [
        "In order to get the text data from the response we need to apply the .text method, and we can save the results in a new varibale hltm_string. The results from the data request will be in [HTML format](https://www.udacity.com/blog/2021/04/html-for-dummies.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "332e8e97",
      "metadata": {
        "id": "332e8e97"
      },
      "outputs": [],
      "source": [
        "html_string = response.text\n",
        "print(html_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ab8e364",
      "metadata": {
        "id": "9ab8e364"
      },
      "source": [
        "Let's bring in our [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) Python library to help us clean up and decode this HTML text data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ccb84c",
      "metadata": {
        "id": "b2ccb84c"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7179fc08",
      "metadata": {
        "id": "7179fc08"
      },
      "source": [
        "Let's run our html_string variable through the Beautiful Soup object and use the get_text() function to extract the text from the HTML data. Then, let's use the print function to visualize our results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18dd67e6",
      "metadata": {
        "scrolled": false,
        "id": "18dd67e6"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(html_string)\n",
        "article = soup.get_text()\n",
        "print(article)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45aee239",
      "metadata": {
        "id": "45aee239"
      },
      "source": [
        "Let's save our results in a text file for future use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f128f6a2",
      "metadata": {
        "id": "f128f6a2"
      },
      "outputs": [],
      "source": [
        "with open(f\"{path}/article.txt\",\"w\") as file:\n",
        "    file.write(article)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e3f1fc",
      "metadata": {
        "id": "03e3f1fc"
      },
      "source": [
        "You can check your \"web_scraping_workshop\" folder to make sure the \"article.txt\" file was successfully saved:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a72c9f5",
      "metadata": {
        "id": "2a72c9f5"
      },
      "outputs": [],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "913a20b8",
      "metadata": {
        "id": "913a20b8"
      },
      "source": [
        "Success! Congrats on scraping your first news article!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afee8871",
      "metadata": {
        "id": "afee8871"
      },
      "source": [
        "## Web Scraping a sequence of URLs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6b7eae",
      "metadata": {
        "id": "df6b7eae"
      },
      "source": [
        "We are now going to learn how to scrape text from a collection of URLs saved as a CSV file. We will use the database of articles we collected during our API workshop."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64d37d04",
      "metadata": {
        "id": "64d37d04"
      },
      "source": [
        "In order to use this dataset, we need to bring it into our Python environment. For this we will use the Pandas library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2c3d51",
      "metadata": {
        "id": "bc2c3d51"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f209119e",
      "metadata": {
        "id": "f209119e"
      },
      "outputs": [],
      "source": [
        "data_df= pd.read_csv(f\"{path}/news_articles.csv\", delimiter=',', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf17a9fd",
      "metadata": {
        "id": "cf17a9fd"
      },
      "outputs": [],
      "source": [
        "data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4651520c",
      "metadata": {
        "id": "4651520c"
      },
      "source": [
        "### Exploring & Cleaning our Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "495e0a22",
      "metadata": {
        "id": "495e0a22"
      },
      "source": [
        "As always, let's examine our dataframe first before we use it to perform any kind of calculation or automation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0190ca0d",
      "metadata": {
        "id": "0190ca0d"
      },
      "source": [
        "Let's explore our data types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671df0a8",
      "metadata": {
        "id": "671df0a8"
      },
      "outputs": [],
      "source": [
        "data_df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dcbf382",
      "metadata": {
        "id": "0dcbf382"
      },
      "source": [
        "It looks like our date is stored as an object (meaning string) instead of a datatime value. Let's convert it (note: the format must match the current format of the data in the column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914ad19e",
      "metadata": {
        "id": "914ad19e"
      },
      "outputs": [],
      "source": [
        "data_df['publishedAt'] = pd.to_datetime(data_df['publishedAt'], format='ISO8601')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e0c330",
      "metadata": {
        "id": "07e0c330"
      },
      "source": [
        "Let's check our data types again to make sure the conversion worked:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc26aa17",
      "metadata": {
        "id": "cc26aa17"
      },
      "outputs": [],
      "source": [
        "data_df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65cda144",
      "metadata": {
        "id": "65cda144"
      },
      "source": [
        "Success! Our data is stored in the proper datatypes!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87ea83e",
      "metadata": {
        "id": "e87ea83e"
      },
      "source": [
        "Let's look at our dataframe again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42fa3e42",
      "metadata": {
        "id": "42fa3e42"
      },
      "outputs": [],
      "source": [
        "data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c06d5d6c",
      "metadata": {
        "id": "c06d5d6c"
      },
      "source": [
        "Notice the NaN values. Pandas has special ways of dealing with missing data. Blank rows in a CSV file show up as NaN in a Pandas DataFrame. </br>\n",
        "- For a cleaner dataset, let's remove those rows with missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed93c666",
      "metadata": {
        "id": "ed93c666"
      },
      "outputs": [],
      "source": [
        "data_df = data_df.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3f7a5d",
      "metadata": {
        "id": "ec3f7a5d"
      },
      "source": [
        "Let's take a look at our Dataframe to make sure we successfully removed our rows with missing values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "237d6acd",
      "metadata": {
        "id": "237d6acd"
      },
      "outputs": [],
      "source": [
        "data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d79b223",
      "metadata": {
        "id": "9d79b223"
      },
      "source": [
        "Success!\n",
        "\n",
        "</br>\n",
        "\n",
        "Finally, let's check for duplicates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed193723",
      "metadata": {
        "id": "ed193723"
      },
      "outputs": [],
      "source": [
        "data_df[data_df.duplicated(keep=False)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fedd4eec",
      "metadata": {
        "id": "fedd4eec"
      },
      "source": [
        "There are no duplicates in our dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34951b3",
      "metadata": {
        "id": "c34951b3"
      },
      "source": [
        "### Automating the Retrieval of Data from URLs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7615d02",
      "metadata": {
        "id": "b7615d02"
      },
      "source": [
        "Each article in this CSV file is paired with a URL. How can we actually use these URLs to get computationally tractable text data?\n",
        "\n",
        "Though we could manually navigate to each URL and copy/paste each article into a file, that would be painstakingly slow, and we would lose crucial data in the process, for example information that might help us automatically distinguish the article headline from the body of the article. It would be much better to programmatically access the text data attached to every URL."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec739d3b",
      "metadata": {
        "id": "ec739d3b"
      },
      "source": [
        "Now that we have a sample dataset, let's set up our code for scraping the text from the list of URLs stored in the URL column. </br>\n",
        "\n",
        "Let's create a new function called scrape_article() that includes our requests.get() and response.text code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0063b656",
      "metadata": {
        "id": "0063b656"
      },
      "outputs": [],
      "source": [
        "def scrape_article(url):\n",
        "    response = requests.get(url)\n",
        "    response.encoding = 'utf-8'\n",
        "    html_string = response.text\n",
        "    return html_string"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32aa0daf",
      "metadata": {
        "id": "32aa0daf"
      },
      "source": [
        "Let's apply our scrape_article function to the “URL” column of the DataFrame and create a new column \"text\" for the resulting extracted text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c0b7e7",
      "metadata": {
        "id": "37c0b7e7"
      },
      "outputs": [],
      "source": [
        "data_df['text'] = data_df['url'].apply(scrape_article)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04590ac",
      "metadata": {
        "id": "f04590ac"
      },
      "source": [
        "Let's take a look at our new dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d0ea76",
      "metadata": {
        "scrolled": true,
        "id": "a1d0ea76"
      },
      "outputs": [],
      "source": [
        "data_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9835830b",
      "metadata": {
        "id": "9835830b"
      },
      "source": [
        "### Retrieving the text from each URL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7a4b286",
      "metadata": {
        "id": "b7a4b286"
      },
      "source": [
        "In the next few steps, we are going to build our for loop that will automate the process of retrieving the text from each URL. We will do this in steps to check what each line in the for loop is doing.\n",
        "\n",
        "</br>\n",
        "\n",
        "Let's start by looking at the data stored in our \"text\" column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb277785",
      "metadata": {
        "id": "bb277785"
      },
      "outputs": [],
      "source": [
        "for text in data_df['text']:\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99893dff",
      "metadata": {
        "id": "99893dff"
      },
      "source": [
        "We can see that our data is in HTML format and is hard to read. Let's run our data through our BeautifulSoup object, apply the get_text() function, and visualize our results using the print function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b03d46",
      "metadata": {
        "id": "73b03d46"
      },
      "outputs": [],
      "source": [
        "for text in data_df['text']:\n",
        "    soup = BeautifulSoup(text)\n",
        "    article = soup.get_text()\n",
        "    print(article)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac39aa31",
      "metadata": {
        "id": "ac39aa31"
      },
      "source": [
        "Now, let's keep building our for loop, and save our data as a text file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca6896a",
      "metadata": {
        "id": "3ca6896a"
      },
      "outputs": [],
      "source": [
        "with open(\"all_articles.txt\",\"w\") as file:\n",
        "    for text in data_df['text']:\n",
        "        soup = BeautifulSoup(text)\n",
        "        article = soup.get_text()\n",
        "        file.write(article)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f54ee8c",
      "metadata": {
        "id": "8f54ee8c"
      },
      "source": [
        "Let's check the \"all_articles.txt\" text file saved in our web_scraping_workshop folder. We have all the articles saved in one doc. This may be a useful way to save our data for some forms of analysis. That being said, if we plan on running any type of comparative analysis, we will need to have them saved as separate files.\n",
        "\n",
        "</br>\n",
        "\n",
        "Let's start by creating a new folder, \"files\", inside our web_scraping_workshop folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b42f79",
      "metadata": {
        "id": "a8b42f79"
      },
      "outputs": [],
      "source": [
        "! mkdir $path/files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf42daf",
      "metadata": {
        "id": "5bf42daf"
      },
      "source": [
        "Next, let's build on our for loop and create a file naming schema to save each article as an individual text file. To do that, we are going to create an index (id =) that starts at 0 and use an f-string (f\") to iterate the file naming process across our list of articles using the index to add a number:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02d9ea1",
      "metadata": {
        "id": "c02d9ea1"
      },
      "outputs": [],
      "source": [
        "id = 0\n",
        "for text in data_df['text']:\n",
        "    soup = BeautifulSoup(text)\n",
        "    article = soup.get_text()\n",
        "\n",
        "    id += 1\n",
        "    with open(f\"{path}/files/article_{id}.txt\", \"w\") as file:\n",
        "        file.write(str(article))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49f25ca",
      "metadata": {
        "id": "d49f25ca"
      },
      "source": [
        "Let's check our files folder to see if our results were sucessfully saved..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8110ec5",
      "metadata": {
        "id": "d8110ec5"
      },
      "outputs": [],
      "source": [
        "! ls $path/files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbaeeb5",
      "metadata": {
        "id": "cfbaeeb5"
      },
      "source": [
        "Congrats, you just run your first autmation loop to web scrape a list of articles!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b7550d0",
      "metadata": {
        "id": "6b7550d0"
      },
      "source": [
        "## Text Cleaning and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c48325e",
      "metadata": {
        "id": "7c48325e"
      },
      "source": [
        "Now that we have our data saved in individual text files, we can run through the process of normalizing and cleaning our data. This includes making the text lowercase, stripping punctuation, and lemmatizing.\n",
        "\n",
        "</br>\n",
        "\n",
        "Once we are done normalizing and cleaning our data, we can then visualize our results in Word Clouds and run a lexical density analysis.\n",
        "\n",
        "Let's start by importing all of our libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d14e68",
      "metadata": {
        "id": "69d14e68"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stops = stopwords.words('english')\n",
        "\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4ebd9c8",
      "metadata": {
        "id": "c4ebd9c8"
      },
      "source": [
        "Next, let's create two new folders within the \"web_scraping_workshop\" folder: one called \"files_cleaned\" where we will save our normalized and cleaned files and another called \"wordclouds\" where we will save our word cloud outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e53bc0d",
      "metadata": {
        "id": "9e53bc0d"
      },
      "outputs": [],
      "source": [
        "! mkdir $path/files_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "187670e4",
      "metadata": {
        "id": "187670e4"
      },
      "outputs": [],
      "source": [
        "! mkdir $path/wordclouds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2347acb0",
      "metadata": {
        "id": "2347acb0"
      },
      "source": [
        "Let's use the Glob library to connect to our \"files\" directory and set it equal to the variable \"files.\" This will turn our file directory into a list of filepaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a01d07ea",
      "metadata": {
        "id": "a01d07ea"
      },
      "outputs": [],
      "source": [
        "directory = f\"{path}/files\"\n",
        "files = glob.glob(f\"{directory}/*.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eb8d332",
      "metadata": {
        "id": "9eb8d332"
      },
      "source": [
        "Let's take a look at our \"files\" variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9119154",
      "metadata": {
        "id": "c9119154"
      },
      "outputs": [],
      "source": [
        "files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fedffdeb",
      "metadata": {
        "id": "fedffdeb"
      },
      "source": [
        "We can see the contents of our \"files\" directory as a list."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2e912de",
      "metadata": {
        "id": "f2e912de"
      },
      "source": [
        "Let's create a function to derive the Part of Speech (POS) of given words. We will use this function to lemmatize our text based on the part of speech (POS) tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b676ba19",
      "metadata": {
        "id": "b676ba19"
      },
      "outputs": [],
      "source": [
        "# Fcn source: https://medium.com/codex/a-beginners-guide-to-easily-create-a-word-cloud-in-python-7c3078c705b7\n",
        "# and https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5ee0d8",
      "metadata": {
        "id": "0f5ee0d8"
      },
      "source": [
        "Let's build a for loop to clean the files saved in our \"files\" variable.\n",
        "We have a few steps:\n",
        "- Create an index (id =) that starts at 0 and use an f-string (f\") to iterate the file naming process across our list of articles using the index to add a number\n",
        "- We start begin our for loop by telling Python to go through each item in the \"files\" list\n",
        "- For each file path, we want Python to do the following:\n",
        "    - open the text file attached to the filepath and set it equal to the variable \"text\"\n",
        "    - transform the words in variable \"text\" into tokens and set it equal to the variable \"text_tokens\"\n",
        "    - process \"text_tokens\" for use with NLTK and set it equal to the variable \"nltk_text\"\n",
        "    - the next three steps make the tokens lower case and removes punctuation (text_lower), removes stop words (text_stops), and lemmatizes the tokens (text_clean)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c42809e",
      "metadata": {
        "id": "9c42809e"
      },
      "outputs": [],
      "source": [
        "id = 0\n",
        "lexical_density = []\n",
        "for filepath in files:\n",
        "    text = open(filepath, encoding='utf-8').read()\n",
        "    text_tokens = nltk.word_tokenize(text)\n",
        "    nltk_text = nltk.Text(text_tokens)\n",
        "    text_lower = [t.lower() for t in nltk_text if t.isalnum()]\n",
        "    text_stops = [t for t in text_lower if t not in stops]\n",
        "    text_clean = [WordNetLemmatizer().lemmatize(t, get_wordnet_pos(t)) for t in text_stops]\n",
        "\n",
        "# save cleaned files\n",
        "\n",
        "    id += 1\n",
        "    with open(f\"{path}/files_cleaned/article_cleaned_{id}.txt\", \"w\") as file:\n",
        "        file.write(str(text_clean))\n",
        "\n",
        "# create Word Clouds\n",
        "    unique_string=(\" \").join(text_clean)\n",
        "    wordcloud = WordCloud(max_font_size=40).generate(unique_string)\n",
        "    plt.figure()\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# save Word Clouds\n",
        "\n",
        "    id += 1\n",
        "    wordcloud.to_file(f\"{path}/wordclouds/word_cloud_{id}.png\")\n",
        "\n",
        "# Establish lexical density\n",
        "    text_clean_slice = text_clean [0:600]\n",
        "    ld_results = len(set(text_clean_slice)) / len(text_clean_slice)\n",
        "    print(ld_results)\n",
        "    ld_dict = {'File_name': filepath, 'lexical_density': ld_results}\n",
        "    lexical_density.append(ld_dict)\n",
        "\n",
        "print(lexical_density)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "800d3366",
      "metadata": {
        "id": "800d3366"
      },
      "source": [
        "We can visualize the results of our lexical density analysis in a dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2083558",
      "metadata": {
        "id": "c2083558"
      },
      "outputs": [],
      "source": [
        "ld_df = pd.DataFrame(lexical_density)\n",
        "ld_df = ld_df.sort_values(by='File_name', ascending=True)\n",
        "ld_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c5ab74",
      "metadata": {
        "id": "34c5ab74"
      },
      "source": [
        "Congrats on making it to the end of this workshop!\n",
        "</br>\n",
        "We've only scratched the surface in terms of the web scraping capabilities of Beautiful Soup. You could run a script to collect all the links of a web page. I've even used it to automatical download an archive of PDF files hosted on a URL.\n",
        "</br>\n",
        "Happy coding!"
      ]
    }
  ],
  "metadata": {
    "gist": {
      "data": {
        "description": "web_scraping.ipynb",
        "public": true
      },
      "id": ""
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}